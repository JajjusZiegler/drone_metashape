{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import pandas, datetime, os, and functions from ScapeFileLocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ScapeFileLocations import populate_site_identification_rules, scrape_metashape_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants\n",
    "Define constants such as MICA_DIR, P1_DIR, EXCEL_FILE, and SHEET_INDEX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "MICA_DIR = r\"M:\\working_package_2\\2024_dronecampaign\\01_data\\Micasense\"\n",
    "P1_DIR = r\"M:\\working_package_2\\2024_dronecampaign\\01_data\\P1\"\n",
    "EXCEL_FILE = r\"C:\\Users\\admin\\Downloads\\UPSCALE_drone_logbook(1).xlsx\"  # Update this with the actual path to your Excel file\n",
    "SHEET_INDEX = 4  # Use 0-based index for the sheet in the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names: ['Gradient_and_paired_sites', 'SiteInfoDrone', 'site_information', 'drone_logbook', '2025_drone_logbook']\n"
     ]
    }
   ],
   "source": [
    "# Load the Excel file to inspect its contents\n",
    "excel_info = pd.ExcelFile(EXCEL_FILE)\n",
    "\n",
    "# Print the sheet names to identify the correct sheet index\n",
    "print(\"Sheet names:\", excel_info.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers in the chosen sheet: ['date', 'month', 'site', 'pilot', 'uav', 'sn', 'gimbal', 'sensor1', 'sensor2', 'rtk_system', 'application', 'height', 'speed [m/s]', 'side_overlap', 'front_overlap', 'start', 'end', 'flight_duration (hh:mm)', 'weather', 'wind', 'remarks', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Site_list']\n"
     ]
    }
   ],
   "source": [
    "# Load the sheet into a DataFrame using the chosen sheet index\n",
    "sheet_df = pd.read_excel(EXCEL_FILE, sheet_name=SHEET_INDEX)\n",
    "\n",
    "# Print the headers of the chosen sheet\n",
    "print(\"Headers in the chosen sheet:\", sheet_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Excel Data\n",
    "Load the Excel file into a DataFrame, clean the date column, and filter relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Clean Excel Data\n",
    "\n",
    "# Define a function to clean the date column\n",
    "def clean_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, dayfirst=True).strftime('%Y%m%d')  # Convert to 'YYYYMMDD' format\n",
    "    except:\n",
    "        return None  # Return None for invalid dates\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(EXCEL_FILE, sheet_name=SHEET_INDEX)\n",
    "\n",
    "# Clean the 'date' column and apply the cleaning function\n",
    "df['date'] = df['date'].apply(clean_date)\n",
    "\n",
    "# Filter relevant columns and drop rows with missing values\n",
    "df = df[['date', 'site', 'weather']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Site Identification Rules\n",
    "Use the populate_site_identification_rules function and scrape_metashape_files to prepare site rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Site Identification Rules\n",
    "\n",
    "# Use the populate_site_identification_rules function to generate initial site rules\n",
    "site_rules = populate_site_identification_rules(MICA_DIR, P1_DIR)\n",
    "\n",
    "# Use scrape_metashape_files to load manual overrides and update site rules\n",
    "scrape_metashape_files(\"\", \"\")  # Triggers rule loading and override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function to Match Site Name to Folder Paths\n",
    "Define the match_site_to_paths function to match site names to folder paths using the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Match Site Name to Folder Paths\n",
    "def match_site_to_paths(site_name, date_str, rules, micasense_dir=MICA_DIR, p1_dir=P1_DIR):\n",
    "    \"\"\"\n",
    "    Matches a site name to its corresponding folder paths for multispectral and RGB images.\n",
    "\n",
    "    Parameters:\n",
    "        site_name (str): The name of the site.\n",
    "        date_str (str): The date string in 'YYYYMMDD' format.\n",
    "        rules (dict): A dictionary containing site identification rules.\n",
    "        micasense_dir (str): The base directory for Micasense data.\n",
    "        p1_dir (str): The base directory for P1 data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the multispectral and RGB folder paths.\n",
    "    \"\"\"\n",
    "    multispec_path, rgb_path = \"Folder not found\", \"Folder not found\"\n",
    "    for canonical, rule in rules.items():\n",
    "        if site_name.strip() in rule['project_name_variants']:\n",
    "            multispec_path = os.path.join(micasense_dir, rule['image_site_name'], date_str)\n",
    "            rgb_path = os.path.join(p1_dir, rule['image_site_name'], date_str)\n",
    "            break\n",
    "    return multispec_path, rgb_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output DataFrame\n",
    "Iterate through the cleaned DataFrame, match site names to paths, and save the output to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Output DataFrame\n",
    "\n",
    "# Initialize an empty list to store the output data\n",
    "output = []\n",
    "\n",
    "# Iterate through each row in the cleaned DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    date_str, site_name, weather = row['date'], row['site'], row['weather']\n",
    "    \n",
    "    # Match site name to folder paths using the defined function\n",
    "    multispec, rgb = match_site_to_paths(site_name, date_str, site_rules)\n",
    "    \n",
    "    # Append the matched data to the output list\n",
    "    output.append({\n",
    "        \"date\": date_str,\n",
    "        \"site\": site_name,\n",
    "        \"weather\": weather,\n",
    "        \"multispec\": multispec,\n",
    "        \"rgb\": rgb\n",
    "    })\n",
    "\n",
    "# Convert the output list to a DataFrame\n",
    "result_df = pd.DataFrame(output)\n",
    "\n",
    "# Save the resulting DataFrame to a CSV file\n",
    "result_df.to_csv(\"flight_log_with_paths.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo)",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
